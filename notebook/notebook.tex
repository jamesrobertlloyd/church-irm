\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

\usepackage{listings}
%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{amssymb,amsmath}
%\usepackage{graphicx}
\usepackage{preamble}
%\usepackage{natbib}
%%%% REMEMBER ME!
%\usepackage[draft]{hyperref}
\usepackage{hyperref}
\usepackage{color}
\usepackage{url}
%\usepackage{wasysym}
%\usepackage{subfigure}
%\usepackage{tabularx}
%\usepackage{booktabs}
%\usepackage{bm}
%\newcommand{\theHalgorithm}{\arabic{algorithm}}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}

\setlength{\marginparwidth}{0.6in}
\input{include/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

% Definitions of handy macros can go here

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

%\jmlrheading{Volume}{Year}{Pages}{Submitted}{Published}{James Robert Lloyd}

% Short headings should be running head and authors last names

\ShortHeadings{Parish records}{Lloyd et alia}
\firstpageno{1}

\begin{document}

\lstset{language=Lisp,basicstyle=\ttfamily\footnotesize} 

\title{Variations on the IRM: Parish Records}

\author{\name James Robert Lloyd \email jrl44@cam.ac.uk \\
       \addr Department of Engineering\\
       University of Cambridge\\
       \AND
       \name Others\dots}

\editor{Editor}

\maketitle

\begin{abstract}
Notes collected whilst testing the latest version of Church.
\end{abstract}

%\begin{keywords}
%  Probabilistic Programming, Church, Relational Data
%\end{keywords}

\section{Introduction}

The latest version of Church (called Venture) has many rough edges.
All results must therefore be taken with a pinch of salt.

Venture relies on computationally efficient but statistically inefficient (as far as I understand) Metropolis--Hastings (MH) based MCMC.
Consequently, great care must be taken over collecting enough samples and averaging over restarts.
Also, absolute numbers of samples are likely to be incomparable between models; I will use measures of effective sample size.

Source code is available at \url{https://github.com/jamesrobertlloyd/church-irm}, which details all experiment parameters.

\section{Multiplicative IRM 20-Feb-2013}

To test the Venture engine and familiarise myself, I wrote a basic version of the IRM \citep{Kemp2006} defined as follows.
%
\begin{lstlisting}[frame=single]
(ASSUME alpha (uniform-continuous 0.0001 2.0))
(ASSUME cluster-crp (CRP/make alpha))
(ASSUME node->class (mem (lambda (node) (cluster-crp))))
(ASSUME classes->parameters (mem (lambda (class1 class2) (beta 0.5 0.5)))) 
(ASSUME p-friends 
  (lambda (node1 node2) (classes->parameters (node->class node1) 
                                             (node->class node2) ) )) 
\end{lstlisting}
%
\ie CRP distributed node classes and beta distributed class interaction probabilities.
Note this is an asymmetric version of the IRM (\eg suitable for directed graphs); a symmetric version is simple to code but requires implementing minimum and maximum functions.
These are not yet defined by the language and user defined versions may be very slow.
Fot the moment I am doubling the data whenever it is symmetric.

Due to some temporary language limitations, I experimented with the only more complex model that could be written sensibly; a multiplicative ensemble of IRMs.
Specifically, I independently repeated the generative process above $D$ times, taking the product of the link probabilities to give the final link probability.
I performed 5 fold cross validation on a high school social network data set (90 nodes, see \eg \cite{Hoff2007a}), computing the AUC of the predictions and the average effective sample size of the predictions (using the batch means method described in \cite{thompson2010comparison}).
500 burn-in and 1000 samples were aimed for, with 100 intermediate MH steps per query to Venture.
Computation time was limited to 30 minutes for both burn in and sampling; the data below shows that this maximum was reached in all but one experiment.
I also include results from a MATLAB implementation (slightly different setup of parameters but quite similar) which I ran for 50 burn in and 100 samples with 5 intermediate gibbs updates per sample.
Raw results are as follows:
%
\begin{lstlisting}
D = 1, fold = 1, ess = 72, AUC = 0.682, Runtime = 3604
D = 1, fold = 2, ess = 61, AUC = 0.688, Runtime = 3604
D = 1, fold = 3, ess = 53, AUC = 0.743, Runtime = 3607
D = 1, fold = 4, ess = 64, AUC = 0.785, Runtime = 3607
D = 1, fold = 5, ess = 68, AUC = 0.648, Runtime = 3605
D = 2, fold = 1, ess = 56, AUC = 0.745, Runtime = 3434
D = 2, fold = 2, ess = 52, AUC = 0.769, Runtime = 3607
D = 2, fold = 3, ess = 52, AUC = 0.767, Runtime = 3607
D = 2, fold = 4, ess = 87, AUC = 0.677, Runtime = 3607
D = 2, fold = 5, ess = 58, AUC = 0.725, Runtime = 3606
D = 3, fold = 1, ess = 54, AUC = 0.796, Runtime = 3607
D = 3, fold = 2, ess = 60, AUC = 0.791, Runtime = 3607
D = 3, fold = 3, ess = 61, AUC = 0.693, Runtime = 3609
D = 3, fold = 4, ess = 65, AUC = 0.748, Runtime = 3608
D = 3, fold = 5, ess = 62, AUC = 0.714, Runtime = 3607

MATLAB fold = 1,           AUC = 0.76,  Runtime = < minute
MATLAB fold = 2,           AUC = 0.75,  Runtime = < minute
MATLAB fold = 3,           AUC = 0.81,  Runtime = < minute
MATLAB fold = 4,           AUC = 0.79,  Runtime = < minute
MATLAB fold = 5,           AUC = 0.79,  Runtime = < minute
\end{lstlisting}
%
Averaging over folds, we get
%
\begin{lstlisting}
D = 1, ess = 64, AUC = 0.710
D = 2, ess = 61, AUC = 0.737
D = 3, ess = 60, AUC = 0.749

MATLAB           AUC = 0.780
\end{lstlisting}
%

\subsection{Conclusions}

These are reasonable AUCs but there is high variance and room for improvement compared to the MATLAB implementation.
The runtime figures are slightly unfair to Venture (low end single core compared to high end hex core) but the difference in efficieny is at least an order of magnitude.
The variance of the raw AUCs suggests that we need much more averaging to draw strong conclusions from empirical data.
The small effective sample sizes suggest that autocorrelation is very high and number of samples (before adjustment for autocorrelation) is likely to only be a measure of memory usage.

There is the slightest hint of a trend in the AUCs; certainly worth investigating.

\subsection{Next steps}

\begin{itemize}
\item Recreate performance of MATLAB sampler with Venture - how much computing power is required?
\item Extend to higher degree products
\item Create small synthetic/real data sets to make testing more robust
\item Add random restarts - how many / what ratios?
\item Experiment with adding randomness to parameters - is it beneficial or does the sampler get stuck?
\end{itemize}

\section{Additive IRM 21-Feb-2013}

Next, an additive IRM.
%
\begin{lstlisting}[frame=single]
(ASSUME alpha (uniform-continuous 0.0001 2.0))
(ASSUME cluster-crp (CRP/make alpha))
(ASSUME node->class (mem (lambda (node) (cluster-crp))))
(ASSUME classes->weights (mem (lambda (class1 class2) (normal 0 1))))
(ASSUME bias (normal 0 2))
(ASSUME p-friends 
  (lambda (node1 node2) 
    (logistic (+ bias (classes->weights (node->class node1) 
                                        (node->class node2) ))) ) ) 
\end{lstlisting}
%
\ie we have replaced each beta with thes sum of a bias and Gaussian passed through a logistic function.

This can become additive by extending the sum within the logistic to contain independent terms with the same form of generative process.
The results are as follows:
%
\begin{lstlisting}
D = 1, fold = 1, ess = 54, AUC = 0.663739, Runtime = 3612
D = 1, fold = 2, ess = 42, AUC = 0.722340, Runtime = 3609
D = 1, fold = 3, ess = 23, AUC = 0.733478, Runtime = 3609
D = 1, fold = 4, ess = 63, AUC = 0.792883, Runtime = 3612
D = 1, fold = 5, ess = 42, AUC = 0.744149, Runtime = 3614
D = 2, fold = 1, ess = 35, AUC = 0.771422, Runtime = 3614
D = 2, fold = 2, ess = 32, AUC = 0.739856, Runtime = 3611
D = 2, fold = 3, ess = 32, AUC = 0.749786, Runtime = 3610
D = 2, fold = 4, ess = 31, AUC = 0.760541, Runtime = 3611
D = 2, fold = 5, ess = 31, AUC = 0.685581, Runtime = 3609
D = 3, fold = 1, ess = 28, AUC = 0.769533, Runtime = 3612
D = 3, fold = 2, ess = 31, AUC = 0.709673, Runtime = 3614
D = 3, fold = 3, ess = 30, AUC = 0.754313, Runtime = 3612
D = 3, fold = 4, ess = 35, AUC = 0.773020, Runtime = 3615
D = 3, fold = 5, ess = 31, AUC = 0.706993, Runtime = 3607
\end{lstlisting}
%
Averaging over folds:%
\begin{lstlisting}
D = 1, ess = 45, AUC = 0.731
D = 2, ess = 32, AUC = 0.741
D = 3, ess = 31, AUC = 0.743
\end{lstlisting}
%
\subsection{Conclusions}

Again, there is a slight hint of a trend but randomness dominates - restarts definitely required.
This model is harder (or rather slower) to sample from as evidenced by the reduced effective sample sizes.
The effective sample sizes are very small  - a lot of computing power will be required.

\subsection{Next steps}

Same conclusions as for product of IRMs.

\section{Parallelised random restarts for basic IRM 21-Feb-2013}

The model
%
\begin{lstlisting}[frame=single]
(ASSUME cluster-crp (CRP/make 1))
(ASSUME node->class (mem (lambda (node) (cluster-crp))))
(ASSUME classes->parameters (mem (lambda (class1 class2) (beta 0.5 0.5)))) 
(ASSUME p-friends 
  (lambda (node1 node2) (classes->parameters (node->class node1) 
                                             (node->class node2) ) )) 
\end{lstlisting}
%
\ie simplified CRP with constant concentration parameter.
This is trying to be more like the MATLAB IRM implementation; to recreate the setup fully I would need to set the beta parameters to 0.1 but this currently causes numerical issues (probably - either way it crashes) in Venture.
Half the computational budget was used for burn in, half for sampling.
The numerical results for the first fold of the highschool data set are as follows:
%
\begin{lstlisting}
D = 1, fold = 1, restart =  1, ess = 38, AUC = 0.683, Runtime = 1804
D = 1, fold = 1, restart =  2, ess = 41, AUC = 0.817, Runtime = 1805
D = 1, fold = 1, restart =  3, ess = 38, AUC = 0.739, Runtime = 1806
D = 1, fold = 1, restart =  4, ess = 42, AUC = 0.651, Runtime = 1806
D = 1, fold = 1, restart =  5, ess = 42, AUC = 0.780, Runtime = 1806
D = 1, fold = 1, restart =  6, ess = 41, AUC = 0.668, Runtime = 1803
D = 1, fold = 1, restart =  7, ess = 35, AUC = 0.781, Runtime = 1805
D = 1, fold = 1, restart =  8, ess = 45, AUC = 0.757, Runtime = 1805
D = 1, fold = 1, restart =  9, ess = 39, AUC = 0.781, Runtime = 1806
D = 1, fold = 1, restart = 10, ess = 33, AUC = 0.811, Runtime = 1805
Total cpu time = 5.02 hours
Local time elapsed = 0.65 hours
D = 1, fold = 1, restarts = 10, ess sum = 399, AUC = 0.795589
\end{lstlisting}
%
\ie brute force can equal the performance of a hand crafted sampler!

Below are results when the computational budget was halved.
Clearly some of the chains have failed to burn in but overall performance was still good.
%
\begin{lstlisting}
D = 1, fold = 1, restart =  1, ess = 28, AUC = 0.756, Runtime = 904
D = 1, fold = 1, restart =  2, ess = 18, AUC = 0.476, Runtime = 903
D = 1, fold = 1, restart =  3, ess = 25, AUC = 0.795, Runtime = 904
D = 1, fold = 1, restart =  4, ess = 28, AUC = 0.694, Runtime = 904
D = 1, fold = 1, restart =  5, ess = 35, AUC = 0.773, Runtime = 903
D = 1, fold = 1, restart =  6, ess = 23, AUC = 0.772, Runtime = 905
D = 1, fold = 1, restart =  7, ess = 28, AUC = 0.805, Runtime = 903
D = 1, fold = 1, restart =  8, ess = 28, AUC = 0.591, Runtime = 903
D = 1, fold = 1, restart =  9, ess = 24, AUC = 0.491, Runtime = 903
D = 1, fold = 1, restart = 10, ess = 23, AUC = 0.778, Runtime = 904
Total cpu time = 2.51 hours
Local time elapsed = 0.30 hours
D = 1, fold = 1, restarts = 10, ess = 265, AUC = 0.782614
\end{lstlisting}
%

\subsection{Next steps}

\begin{itemize}
\item Run other data folds to be sure of the result
\item Try to reduce total computation time - what is the `best' number of random restarts
\item Produce stats on memory usage and data transfer - need to be careful! 
\item Start costing the use of the cloud and make an arrangement with Venture
\end{itemize}

\section{Parallelised multiplicative IRM 28-Feb-2013}

Extending the experiment above to the multiplicative IRM we get the following results:
%
\begin{lstlisting}
D = 2, fold = 1, restart =  1, ess = 44, AUC = 0.796, Runtime = 1803
D = 2, fold = 1, restart =  2, ess = 97, AUC = 0.790, Runtime = 1805
D = 2, fold = 1, restart =  3, ess = 42, AUC = 0.749, Runtime = 1805
D = 2, fold = 1, restart =  4, ess = 52, AUC = 0.689, Runtime = 1805
D = 2, fold = 1, restart =  5, ess = 42, AUC = 0.779, Runtime = 1804
D = 2, fold = 1, restart =  6, ess = 49, AUC = 0.760, Runtime = 1804
D = 2, fold = 1, restart =  7, ess = 41, AUC = 0.736, Runtime = 1804
D = 2, fold = 1, restart =  8, ess = 99, AUC = 0.753, Runtime = 1804
D = 2, fold = 1, restart =  9, ess = 48, AUC = 0.753, Runtime = 1804
D = 2, fold = 1, restart = 10, ess = 41, AUC = 0.780, Runtime = 1804
Total cpu time = 5.01 hours
Local time elapsed = 0.70 hours
D = 2, fold = 1, restarts = 10, ess = 559, AUC = 0.814
\end{lstlisting}
%
\begin{lstlisting}
D = 3, fold = 1, restart =  1, ess = 35, AUC = 0.808, Runtime = 723
D = 3, fold = 1, restart =  2, ess = 24, AUC = 0.701, Runtime = 726
D = 3, fold = 1, restart =  3, ess = 23, AUC = 0.651, Runtime = 725
D = 3, fold = 1, restart =  4, ess = 26, AUC = 0.663, Runtime = 725
D = 3, fold = 1, restart =  5, ess = 33, AUC = 0.658, Runtime = 725
D = 3, fold = 1, restart =  6, ess = 28, AUC = 0.688, Runtime = 724
D = 3, fold = 1, restart =  7, ess = 26, AUC = 0.691, Runtime = 725
D = 3, fold = 1, restart =  8, ess = 26, AUC = 0.674, Runtime = 724
D = 3, fold = 1, restart =  9, ess = 26, AUC = 0.649, Runtime = 724
D = 3, fold = 1, restart = 10, ess = 29, AUC = 0.725, Runtime = 725
Total cpu time = 2.01 hours
Local time elapsed = 0.23 hours
D = 3, fold = 1, restarts = 10, ess = 281, AUC = 0.771
\end{lstlisting}
%
It appears the twice multiplicative model is easier to sample (higher ess) and achieves a higher AUC.
The thrice multiplicative experiment has performed poorly but this had higher memory requirements and therefore needed to be run on a different type of core (hence the reduced runtime to offset the increase in performance).
Trying to offset the increase in performance appears not to have worked.
So:
%
\begin{lstlisting}
D = 3, fold = 1, restart =  1, ess = 38, AUC = 0.619, Runtime = 1805
D = 3, fold = 1, restart =  2, ess = 42, AUC = 0.758, Runtime = 1805
D = 3, fold = 1, restart =  3, ess = 42, AUC = 0.775, Runtime = 1804
D = 3, fold = 1, restart =  4, ess = 59, AUC = 0.672, Runtime = 1806
D = 3, fold = 1, restart =  5, ess = 52, AUC = 0.700, Runtime = 1804
D = 3, fold = 1, restart =  6, ess = 43, AUC = 0.779, Runtime = 1807
D = 3, fold = 1, restart =  7, ess = 42, AUC = 0.649, Runtime = 1805
D = 3, fold = 1, restart =  8, ess = 46, AUC = 0.653, Runtime = 1805
D = 3, fold = 1, restart =  9, ess = 48, AUC = 0.710, Runtime = 1805
D = 3, fold = 1, restart = 10, ess = 41, AUC = 0.694, Runtime = 1805
Total cpu time = 5.01 hours
Local time elapsed = 0.50 hours
D = 3, fold = 1, restarts = 10, ess = 457, AUC = 0.769
\end{lstlisting}
%
Suggesting that the reduced performance is true.
However, more experiments are required to draw strong conclusions.

\subsection{Next steps}

\begin{itemize}
\item Run extensive experiemnts on small data sets, recording many diagnostics to understand both the model and Venture
\end{itemize}

\section{First comparison of multiplicative IRM 06-Mar-2013}

The model is the product of $D$ IRMs, with either fixed concentration and beta parameters or uniform / gamma priors on them respectively (details in source code).
I generated synthetic data with 20 nodes from the prior of the product IRM with $D = 1,2,3$.
A brief summary of results are below:

\begin{table*}[ht!]
\caption{{\small
Product IRM comparison - AUCs
}}
\label{tbl:Product IRM 06-Mar-2013}
\begin{center}
\begin{tabular}{c c | r r r}
Model & $\alpha, \beta$ & synth $D=1$ & synth $D=2$ & synth $D=3$ \\
\hline
$D=1$ & fixed & 0.743 &  0.679 &  0.645 \\
$D=2$ & fixed & 0.708 &  0.689 &  0.649 \\
$D=3$ & fixed & 0.735 &  0.713 &  0.618 \\
$D=4$ & fixed & 0.755 &  0.671 &  0.614 \\
$D=5$ & fixed & 0.710 &  0.678 &  0.625 \\
\hline
$D=1$ & random & 0.752 &  0.713 &  0.626 \\
$D=2$ & random & 0.747 &  0.710 &  0.651 \\
$D=3$ & random & 0.725 &  0.668 &  0.610 \\
$D=4$ & random & 0.747 &  0.663 &  0.640 \\
$D=5$ & random & 0.752 &  0.694 &  0.664 \\
\end{tabular}
\end{center}
\end{table*}

Various diagnostics (\eg effective sample size) are excluded - not checked fully at the moment but no major concerns suspected.
Parallelisation of this experiment offered a 20$\times$ speedup, but this can likely be increased by reserving cores on picloud (realtime cores).

\subsection{Conclusions}

\begin{itemize}
\item Data sets are probably too small to discover the structure for $D > 1$
\item Random parameters offer a slight improvement - can probably be increased with better priors
\item Experiments inconclusive about the choice of $D$
\end{itemize}

\subsection{Next steps}

\begin{itemize}
\item Agree financial arrangements with Venture for larger scale experiments
\item Use realtime cores and update memory and cpu stats accordingly
\item Experiment with data set size - how much data do we need to see $D = 2$ performing better than $D = 1$ on synthetic data?
\item Experiment with data set size - how do cpu time and memory usage vary?
\item Experiment with priors on $\alpha$ and beta parameters
\item More data sets, more models
\end{itemize}

\section{Data set size test 08-Mar-2013}

\TBD{
Quick notes: I created synthetic data sets of differeing sizes, hoping to show that at a certain data size one could distinguish between a regular IRM and product of two IRMs.
No statistically significant difference was detected (number of nodes was 30, 60 and 90) prompting tests on real data.
Ultimately we should test the Venture engine on some heterogeous models and data generated from them to check that we are not conflating Venture's ability to sample from certain types of model.}

\section{Multiplicative and additive IRMs 11-Mar-2013}

Using the hyperparameter version of the product IRM described
%
\begin{lstlisting}[frame=single]
(ASSUME alpha (uniform-continuous 0.0001 2.0))
(ASSUME cluster-crp (CRP/make alpha))
(ASSUME node->class (mem (lambda (node) (cluster-crp))))
(ASSUME classes->weights (mem (lambda (class1 class2) 
                              (normal 0 (+ 0.01 (gamma 1 1))))))
(ASSUME bias (normal 0 2))
(ASSUME p-friends 
  (lambda (node1 node2) 
    (logistic (+ bias (classes->weights (node->class node1) 
                                        (node->class node2) ))) ) ) 
\end{lstlisting}
%
where the final sum within the logistic might include multiple independent a priori models.

{
\remark
The gamma prior on the standard deviations of the normals is independent for each weight.
This is a coding error rather than a design choice, leading to a marginally heavy tailed distribution rather than an automatic relevance determination effect.
The same lack of coupling happened for the multiplicative IRM as well.
Future experiments will try the coupled parameter version of these models.
}

Experiments were on an 80/20 split of the first 50 nodes of some `standard' datasets.
AUC results below, effective sample sizes suggest that the experiments were ok.
Protein and NIPS are very sparse when subsetted in this manner so the results are high variance.
Parallelisation issues are essentially fixed; these experiments were run with 100x speedup and cost about \$10.

\begin{table*}[ht!]
\caption{{\small
Product and additive IRM comparisons - AUCs
}}
\label{tbl:Prod Add IRM 11-Mar-2013}
\begin{center}
\begin{tabular}{c | r r r r}
Model & Highschool & Dolphin & Protein & NIPS \\
\hline
$P : D=1$ & 0.806 & 0.684 &  0.692 &  $\mathbf{0.868}$ \\
$P : D=3$ & $\mathbf{0.843}$ & 0.774 &  0.742 &  0.603 \\
$P : D=5$ & 0.819 & $\mathbf{0.784}$ &  $\mathbf{0.784}$ &  0.702 \\
$P : D=7$ & 0.822 & 0.771 &  0.725 &  0.648 \\
\hline
$A : D=1$ & 0.811 & 0.752 &  0.714 &  $\mathbf{0.691}$ \\
$A : D=3$ & $\mathbf{0.825}$ & 0.763 &  0.744 &  0.597 \\
$A : D=5$ & 0.822 & $\mathbf{0.785}$ &  $\mathbf{0.762}$ &  0.591 \\
$A : D=7$ & 0.804 & 0.783 &  0.760 &  0.551 \\
\end{tabular}
\end{center}
\end{table*}

\subsection{Conclusions}

\begin{itemize}
\item No clear winner for additive vs.~multipicative (although there may be a significant difference in predictive likelihoods due to the introduction of an explicit bias term in the additive model.
\item Very interesting that the top scoring models follow the same dimension / data set pattern. This suggests that there is insufficient automatic complexity control (which could have been achieved by hyperparameter coupling - see remark).
\end{itemize}

\subsection{Next steps}

\begin{itemize}
\item Experiment with automatic relevance determination style models
\item Try to write versions which place a prior on the number of additive/multiplicative components
\item Evaluate predictive likelihoods and other performance scores
\end{itemize}

\subsection{Multiplicative and additive IRMs 12-Mar-2013}

I ran some similar experiments (mostly because I realised I could fit them on a cheaper core).
Apologies for flipping the order of NIPS and Dolphins.

\begin{table*}[ht!]
\caption{{\small
Product and additive IRM comparisons - AUCs
}}
\label{tbl:Prod Add IRM 11-Mar-2013}
\begin{center}
\begin{tabular}{c | r r r r}
Model & Highschool & NIPS & Protein & Dolphins \\
\hline
$P : D=1$ & 0.787 & $\mathbf{0.813}$ & 0.688 & 0.674 \\
$P : D=2$ & 0.837 & 0.716 & 0.781 & 0.818 \\
$P : D=3$ & $\mathbf{0.852}$ & 0.648 & $\mathbf{0.796}$ & $\mathbf{0.823}$ \\
$P : D=4$ & 0.815 & 0.700 & 0.764 & 0.814 \\
$P : D=5$ & 0.806 & 0.646 & 0.756 & 0.813 \\
\hline
$A : D=1$ & $\mathbf{0.823}$ & $\mathbf{0.720}$ & 0.712 & 0.754 \\
$A : D=2$ & 0.815 & 0.695 & 0.711 & 0.746 \\
$A : D=3$ & 0.821 & 0.580 & 0.729 & $\mathbf{0.786}$ \\
$A : D=4$ & 0.821 & 0.593 & 0.733 & 0.765 \\
$A : D=5$ & 0.797 & 0.613 & $\mathbf{0.741}$ & 0.757 \\
\end{tabular}
\end{center}
\end{table*}

\subsection{Conclusions}

\begin{itemize}
\item Perhaps some evidence that the multiplicative model is winning
\item No clear structure in the dimensionality other than NIPS preferring one dimension
\end{itemize}

\subsection{Next steps}

\begin{itemize}
\item Experiment with automatic relevance determination style models
\item Try to write versions which place a prior on the number of additive/multiplicative components
\item Evaluate predictive likelihoods and other performance scores
\end{itemize}

\section{Tying hyperparameters 12-Mar-2013}

Fixing the lack of hyperparameter tying, I performed the same experiment with the corrected models.
These priors have the ability to perform automatic relevance determination and thus might be expected to be less senstive to the dimensionality.

\begin{table*}[ht!]
\caption{{\small
Product and additive IRM comparisons - AUCs
}}
\label{tbl:Prod Add IRM 11-Mar-2013}
\begin{center}
\begin{tabular}{c | r r r r}
Model & Highschool & NIPS & Protein & Dolphins \\
\hline
$P : D=1$ & 0.841 & $\mathbf{0.743}$ & 0.717 & 0.766 \\
$P : D=2$ & $\mathbf{0.856}$ & 0.654 & 0.777 & 0.805 \\
$P : D=3$ & 0.822 & 0.660 & 0.762 & 0.791 \\
$P : D=4$ & 0.824 & 0.708 & 0.784 & $\mathbf{0.835}$ \\
$P : D=5$ & 0.839 & 0.716 & $\mathbf{0.794}$ & 0.785 \\
\hline
$A : D=1$ & 0.800 & $\mathbf{0.708}$ & 0.747 & 0.719 \\
$A : D=2$ & 0.810 & 0.646 & 0.741 & $\mathbf{0.775}$ \\
$A : D=3$ & 0.819 & 0.671 & 0.762 & 0.738 \\
$A : D=4$ & $\mathbf{0.840}$ & 0.652 & $\mathbf{0.798}$ & 0.766 \\
$A : D=5$ & 0.810 & 0.654 & 0.745 & 0.769 \\
\end{tabular}
\end{center}
\end{table*}

\subsection{Conclusions}

\begin{itemize}
\item More evidence that multiplicative wins
\item No clear structure in the dimensionality again apart from NIPS
\item Higher dimensional models (4 and 5) almost always (7 out of 8) performing better than previous (no ARD) versions
\item Performance decrease with higher dimensions on NIPS not as severe as before
\item No real evidence that the dimensionality of a model is key; an adaptive model will probably do fine
\end{itemize}

\subsection{Next steps}

\begin{itemize}
\item Try to write versions which place a prior on the number of additive/multiplicative components
\item Evaluate predictive likelihoods and other performance scores
\item Need restarts / folds to asses statistical significance
\end{itemize}

\section{Binary feature models 27-Mar-2013}

As a simple test, I modified the additive IRM model, replacing \texttt{node->class} with a \texttt{node->feature} function that returns true or false.
Note that this is not an LFRM \citep{Miller2009}.
Table~\ref{tbl:Finite 2-ILA 26-Mar-2013} shows the results.

\begin{table*}[ht!]
\caption{{\small
Finite 2-ILA - AUCs
}}
\label{tbl:Finite 2-ILA 26-Mar-2013}
\begin{center}
\begin{tabular}{c | r r r r}
Model & Highschool & NIPS & Protein & Dolphins \\
\hline
1 & 0.779 & 0.809 & 0.636 & 0.706 \\
2 & \textbf{0.830} & 0.817 & 0.713 & \textbf{0.728} \\
3 & 0.810 & \textbf{0.827} & 0.686 & 0.710 \\
4 & 0.821 & 0.634 & \textbf{0.722} & 0.696 \\
5 & 0.822 & 0.599 & 0.686 & 0.698 \\
6 & 0.785 & 0.599 & 0.652 & 0.684
\end{tabular}
\end{center}
\end{table*}

A standard finite approximation to the LFRM was also tried.
The current implementation is very high memory, preventing a reasonable number of features; currently working with Venture team to reduce memory.
At the moment, the results in table~\ref{tbl:LFRM 26-Mar-2013} are fairly poor as a result.

\begin{table*}[ht!]
\caption{{\small
Finite LFRM - AUCs
}}
\label{tbl:LFRM 26-Mar-2013}
\begin{center}
\begin{tabular}{c | r r r r}
Model & Highschool & NIPS & Protein & Dolphins \\
\hline
1 & 0.691 & 0.514 & 0.554 & 0.446 \\
2 & \textbf{0.714} & \textbf{0.560} & 0.644 & \textbf{0.720} \\
3 & 0.697 & 0.551 & \textbf{0.663} & 0.609 \\
4 & 0.694 & 0.566 & 0.629 & 0.671
\end{tabular}
\end{center}
\end{table*}

\subsection{Next steps}

\begin{itemize}
\item Reduce memory requirements of LFRM, a task very much related to\ldots
\item \ldots try to write versions which place a prior on the number of additive/multiplicative components
\end{itemize}

\section{Memories of 28-Mar-2013}

Recursive functions soak up memory very quickly!
Need to work on appropriate sparsity inducing fixed size priors that do not require recursion within Venture.
If they are absolutely necessary, we should try to get them implemented as primitives.
But Dan thinks there is a more worrying underlying problem\ldots

\section{First try on cold start problems 29-Mar-2013}

Data: complete social network and cold start tag data (think Facebook `likes') in the journal article version of AH paper (different fold at the moment).
Trying to see if the hypothesis that additive dominates is true.
Results are mixed but pointing in roughly the correct direction.

First, product IRMs, would appear to be a poor model (10 restarts, 20 minutes of sampling).

\begin{table*}[ht!]
\caption{{\small
Product IRMs on cold start - AUCs
}}
\label{tbl:Prod-IRM 29-Mar-2013}
\begin{center}
\begin{tabular}{c | r}
Model & AUC \\
\hline
1 & 0.505 \\
3 & 0.468 \\
5 & 0.508 
\end{tabular}
\end{center}
\end{table*}

Then some additive models (with long run times since I had scared myself with previous results) had some success.
Inspecting individual AUCs from each repeat showed that there was still a lot of variance in individual AUCs (\ie the chains are not mixing as measured by AUC).

\begin{table*}[ht!]
\caption{{\small
Additive IRMs - cold start - AUCs
}}
\label{tbl:Add-IRM 29-Mar-2013}
\begin{center}
\begin{tabular}{c c c | r}
Dimension & Repeats &  Sample time & AUC\\
\hline
10 & 1 & 5 hours & 0.606 \\
10 & 25 & 50 minutes & 0.646 
\end{tabular}
\end{center}
\end{table*}

\section{More cold start 01-Apr-2013}

I then tried to explore if lots of random restarts was good, or if it was about sample time.
This appears to be suggesting that we cannot take shortcuts with computation, either lots of restarts or lots of sample time is required (and probably preferably both).

\begin{table*}[ht!]
\caption{{\small
Additive IRMs - cold start - AUCs
}}
\label{tbl:Add-IRM 01-Apr-2013}
\begin{center}
\begin{tabular}{c c c | r}
Dimension & Repeats &  Sample time & AUC\\
\hline
1 & 100 & 2 minutes & 0.550 \\
10 & 25 & 2 minutes & 0.495 \\
10 & 10 & 10 minutes & 0.496 \\
10 & 10 & 2 minutes & 0.605 \\
10 & 1 & 5 hours & 0.606 \\
10 & 100 & 2 minutes & 0.612 \\
10 & 25 & 50 minutes & 0.646 
\end{tabular}
\end{center}
\end{table*}

Feeling mildly confident that lots of restarts with small computations might say something useful, I compared product, additive and different dimensionalities; 2 minutes of sampling and 100 restarts.

\begin{table*}[ht!]
\caption{{\small
Additive and product IRMs - cold start - AUCs
}}
\label{tbl:IRM 01-Apr-2013}
\begin{center}
\begin{tabular}{c c | r}
Type & D & AUC\\
\hline
A & 1 & 0.526 \\
A & 3 & 0.451 \\
A & 5 & 0.595 \\
A & 7 & 0.619 \\
P & 1 & 0.453 \\
P & 3 & 0.500 \\
P & 5 & 0.513 \\
P & 7 & 0.514 
\end{tabular}
\end{center}
\end{table*}

Clearly the numbers are noisy, but they do not contradict the initial hypothesis that an additive model would be the better performer on this joint modelling task (since it has the capacity not to be distracted by modelling the social network).

\subsection{Conclusions}

\begin{itemize}
\item Main hypothesis has some evidence in favour of it
\item Computations getting moderately large (memory requirements force using expensive cores)
\end{itemize}

\subsection{Next steps}

\begin{itemize}
\item Run a version of this experiment with five fold validations
\item Start costing larger scale experiments
\end{itemize}

\section{More cold start 02-Apr-2013}

I then extended the experiments to five fold cross validations.
Thinking that many short chains gave an indication of performance I used 2 minutes and 100 restarts.

\begin{table*}[ht!]
\caption{{\small
Additive and product IRMs - cold start - AUCs
}}
\label{tbl:IRM 02-Apr-2013}
\begin{center}
\begin{tabular}{c c | r r r r r | r}
Type & D & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Average \\
\hline
A & 1 & 0.577 & 0.717 & 0.818 & 0.343 & 0.417 & 0.574\\
A & 3 & 0.487 & 0.612 & 0.456 & 0.500 & 0.495 & 0.510\\
A & 5 & 0.547 & 0.558 & 0.520 & 0.573 & 0.595 & 0.559\\
A & 7 & 0.489 & 0.492 & 0.409 & 0.675 & 0.474 & 0.508\\
P & 1 & 0.481 & 0.617 & 0.745 & 0.262 & 0.338 & 0.488\\
P & 3 & 0.478 & 0.721 & 0.610 & 0.477 & 0.507 & 0.559\\
P & 5 & 0.476 & 0.555 & 0.364 & 0.441 & 0.554 & 0.478\\
P & 7 & 0.515 & 0.580 & 0.495 & 0.347 & 0.603 & 0.508\\
\end{tabular}
\end{center}
\end{table*}

It appears I have been tricked by earlier experiments into thinking that many restarts could outweigh small sampling time.

\subsection{Next steps}

\begin{itemize}
\item Sample for longer - will try a few things but a good way to waste money
\item Reduce data set size to have a chance of mixing slightly - probably the best idea
\item Revisit model design - probably best done in conjunction with smaller data sets
\end{itemize}

\section{More cold start 03-Apr-2013}

Based (again) on some encouraging trial runs, I tried the experiment above with 20 minutes and 20 restarts.
Have I been tricked again?

\begin{table*}[ht!]
\caption{{\small
Additive and product IRMs - cold start - AUCs
}}
\label{tbl:IRM 03-Apr-2013}
\begin{center}
\begin{tabular}{c c | r | r r r r r | r}
Type & D & Fold ? & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Average \\
\hline
A & 1 & 0.419 & 0.453 & 0.542 & 0.704 & 0.441 & 0.228 & 0.465\\
A & 7 & 0.578 & 0.561 & 0.545 & 0.644 & 0.403 & 0.466 & 0.533\\
P & 1 & 0.520 & 0.454 & 0.467 & 0.824 & 0.437 & 0.282 & 0.497\\
P & 7 & 0.538 & 0.483 & 0.230 & 0.627 & 0.356 & 0.498 & 0.455\\
\end{tabular}
\end{center}
\end{table*}

It is tempting to think that patterns are emerging but it is far from clear.
The additive and product models are following similar trends across the folds which is encouraging.

Smaller data sets using cheaper cores with more model design is almost certainly the way to go.

\subsection{Next steps}

\begin{itemize}
\item Reduce data set size to have a chance of mixing slightly
\item Revisit model design
\end{itemize}

\section{Smaller cold start 04-Apr-2013}

20 node egocentric network data.
5 restarts, 30 minutes of sampling.

\begin{table*}[ht!]
\caption{{\small
Additive IRMs - cold start - AUCs
}}
\label{tbl:IRM 04-Apr-2013}
\begin{center}
\begin{tabular}{c c | r r r r | r}
Type & D & Batmanchasm & artagnon & paperdre4m & yosefyah & Average \\
\hline
A & 1 & 0.216 & 0.333 & 0.387 & 0.337 & 0.318\\
A & 2 & 0.054 & 0.389 & 0.742 & 0.304 & 0.372\\
A & 3 & 0.270 & 0.444 & 0.435 & 0.380 & 0.383\\
A & 4 & 0.108 & 0.111 & 0.484 & 0.431 & 0.284\\
A & 5 & 0.081 & 0.222 & 0.476 & 0.304 & 0.271\\
\end{tabular}
\end{center}
\end{table*}

Results are still very high variance.

\subsection{Next steps}

\begin{itemize}
\item Show some negative results of \eg collab filtering with and without side information?
\item Stop using Venture?
\item Try 30, 40, 50 node networks?
\end{itemize}

\newpage

%\appendix
%\section*{Appendix A.}
%Appendix

\vskip 0.2in
\bibliography{library}

\end{document}
